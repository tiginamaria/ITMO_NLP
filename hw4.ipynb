{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw4 (3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Soft deadline: `30.03.2022 23:59`"
      ],
      "metadata": {
        "id": "nXpkXz1QJmhJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxy7euTpCbGp"
      },
      "source": [
        "In this homework you will understand the fine-tuning procedure and get acquainted with Huggingface Datasets library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1MXcMymXeCx"
      },
      "source": [
        "! pip install datasets\n",
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FykUK-TFXf-2"
      },
      "source": [
        "For our goals we will use [Datasets](https://huggingface.co/docs/datasets/) library and take `yahoo_answers_topics` dataset - the task of this dataset is to divide documents on 10 topic categories. More detiled information can be found on the dataset [page](https://huggingface.co/datasets/viewer/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebsFAQsgXNB0"
      },
      "source": [
        "from datasets import load_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbzxZi42XOUG"
      },
      "source": [
        "dataset = load_dataset('yahoo_answers_topics') # the result is a dataset dictionary of train and test splits in this case"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U4YUOB5W8uG"
      },
      "source": [
        "# Fine-tuning the model** (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDYIq9l7CYBR"
      },
      "source": [
        "from transformers import (ElectraTokenizer, ElectraForSequenceClassification,\n",
        "                          get_scheduler, pipeline, ElectraForMaskedLM, ElectraModel)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElZ6k36rb0VG"
      },
      "source": [
        "Fine-tuning procedure on the end task consists of adding additional layers on the top of the pre-trained model. The resulting model can be tuned fully (passing gradients through the all model) or partially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNEmksaPb3Uu"
      },
      "source": [
        "**Task**: \n",
        "- load tokenizer and model\n",
        "- look at the predictions of the model as-is before any fine-tuning\n",
        "\n",
        "\n",
        "```\n",
        "- Why don't you ask [MASK]?\n",
        "- What is [MASK]\n",
        "- Let's talk about [MASK] physics\n",
        "```\n",
        "\n",
        "- convert `best_answer` to the input tokens (supporting function for dataset is provided below) \n",
        "\n",
        "```\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"best_answer\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "```\n",
        "\n",
        "- define optimizer, sheduler (optional)\n",
        "- fine-tune the model (write the training loop), plot the loss changes and measure results in terms of weighted F1 score\n",
        "- get the masked word prediction (sample sentences above) on the fine-tuned model, why the results as they are and what should be done in order to change that (write down your answer)\n",
        "- Tune the training hyperparameters (and write down your results).\n",
        "\n",
        "**Tips**:\n",
        "- The easiest way to get predictions is to use transformers `pipeline` function \n",
        "- Do not forget to set `num_labels` parameter, when initializing the model\n",
        "- To convert data to batches use `DataLoader`\n",
        "- Even the `small` version of Electra can be long to train, so you can take data sample (>= 5000 and set seed for reproducibility)\n",
        "- You may want to try freezing (do not update the pretrained model weights) all the layers exept the ones for classification, in that case use:\n",
        "\n",
        "\n",
        "```\n",
        "for param in model.electra.parameters():\n",
        "      param.requires_grad = False\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yqAAFqZcwbu"
      },
      "source": [
        "MODEL_NAME = \"google/electra-small-generator\"\n",
        "TOKENIZER_NAME = \"google/electra-small-generator\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}